{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "\n",
    "\n",
    "fname_parsed = 'ai.ja/ai.ja.txt.parsed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, dct):\n",
    "        self.surface = dct['surface']\n",
    "        self.base = dct['base']\n",
    "        self.pos = dct['pos']\n",
    "        self.pos1 = dct['pos1']\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"surface:'{}',  base: '{}', pos: '{}',  pos1: '{}'\"\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs     # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst                   # 係り先文節インデックス番号\n",
    "        self.srcs = []                    # 係り元文節インデックス番号のリスト\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"morphs:'{}',  dst: '{}', srcs: '{}'\"\\\n",
    "            .format(self.morphs, self.dst, self.srcs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_analysis(paragraph) -> list:\n",
    "    '''ependency_analysis(係り受け解析)の説明\n",
    "    paragraphはcabocha(CaboCha.FORMAT_LATTICE)で解析したあとのパラグラフとする\n",
    "    例 出力例(道具を用いて『知能』を研究する)\n",
    "    形態素のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）\n",
    "    9.　　 ['道具', 'を'] 10 [5, 6, 8]\n",
    "    10. ['用い', 'て'] 12 [9]\n",
    "    11. ['『', '知能', '』', 'を'] 12 []\n",
    "    12. ['研究', 'する'] 13 [10, 11]\n",
    "    '''\n",
    "    morphs =[]\n",
    "    chunks = []\n",
    "    lines = paragraph.split('\\n')\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "             chunks.append(Chunk(morphs, dst))\n",
    "        elif line[0]=='*':\n",
    "            if len(morphs) > 0:\n",
    "                chunks.append(Chunk(morphs, dst))\n",
    "                morphs =[]\n",
    "            dst = int(line.split(' ')[2].rstrip('D'))\n",
    "        else:\n",
    "            cols = line.split('\\t')\n",
    "            cols = [cols[0]] + cols[1].split(',')\n",
    "            dct = {\n",
    "                'surface': cols[0],\n",
    "                'base': cols[7],\n",
    "                'pos': cols[1],\n",
    "                'pos1': cols[2]\n",
    "            }\n",
    "            morphs.append(Morph(dct))\n",
    "            \n",
    "    for  i, chunk in enumerate(chunks):\n",
    "        if chunk.dst != -1:\n",
    "            chunks[chunk.dst].srcs.append(i)\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# in_fileはテキストファイルをcabocha(CaboCha.FORMAT_LATTICE)で解析した後のテキストファイル\n",
    "with open(fname_parsed) as in_file:\n",
    "    paragraphs = in_file.read().split('EOS\\n')\n",
    "    \n",
    "paragraphs = list(filter(lambda x: x!='', paragraphs))\n",
    "\n",
    "# パラグラフごとに係り受け解析をし, リストに格納\n",
    "paragraphs = [dependency_analysis(paragraph) for paragraph in paragraphs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行動を代わる\tを を に\t知的行動を 知的行動を 人間に\n",
      "判断をする\tを を を\t推論判断を 推論判断を 推論判断を\n",
      "処理を用いる\tを を\t記号処理を 記号処理を\n",
      "記述をする\tを と\t記述を 主体と\n",
      "注目を集める\tが が が を\tサポートベクターマシンが サポートベクターマシンが サポートベクターマシンが 注目を\n",
      "学習を行う\tを に を\t経験を 元に 学習を\n",
      "流行を超える\tを\t流行を\n",
      "学習を繰り返す\tを\t学習を\n",
      "学習をする\tを を を に を通して を通して\t統計的学習を 統計的学習を 統計的学習を 元に 生成規則を通して 生成規則を通して\n",
      "進化を見せる\tにおいて において を\t生成技術において 生成技術において 進化を\n",
      "生成を行う\tを を\tコンテンツ生成を コンテンツ生成を\n",
      "開発を行う\tは を\tエイダ・ラブレスは 開発を\n",
      "テストをする\tを\tテストを\n",
      "処理を行う\tを\t処理を\n",
      "処理を行う\tに を を\tWebに 知的処理を 知的処理を\n",
      "意味をする\tに を\tデータに 意味を\n",
      "処理を行う\tに を を\tコンピュータに 知的処理を 知的処理を\n",
      "研究を進める\tて を\t費やして 研究を\n",
      "命令をする\tを で\t命令を 機構で\n",
      "運転をする\tに を\t元に 運転を\n",
      "特許をする\tに に に が を\t2018年までに 2018年までに 2018年までに 日本が 特許を\n",
      "研究をする\tを\t研究を\n",
      "運転をする\tに を\t柔軟に 運転を\n",
      "注目を集める\tは を\tファジィは 注目を\n",
      "制御をする\tを を\tニューロファジィ制御を ニューロファジィ制御を\n",
      "成功を受ける\tを\t成功を\n",
      "制御を用いる\tも を を\t他社も 知的制御を 知的制御を\n",
      "制御をする\tを\t制御を\n",
      "制御をする\tを を\t知的制御を 知的制御を\n",
      "進歩を担う\tを\t進歩を\n",
      "改善を果たす\tに に で で で が を を\t2012年に 2012年に 画像処理コンテストで 画像処理コンテストで 画像処理コンテストで チームが 精度改善を 精度改善を\n",
      "プログラムを使う\tを を\t専用プログラムを 専用プログラムを\n",
      "研究を続ける\tて を\t向けて 研究を\n",
      "*をする\tを を を を を を を\t記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を 記号接地問題(シンボルグラウンディング問題)を\n",
      "注目を集める\tに を\t急速に 注目を\n",
      "普及を受ける\tを\t普及を\n",
      "学習を組み合わせる\tを を\t機械学習を 機械学習を\n",
      "投資を行う\tに に に で で で を\t全世界的に 全世界的に 全世界的に 民間企業主導で 民間企業主導で 民間企業主導で 投資を\n",
      "探索を行う\tで で を\t無報酬で 無報酬で 探索を\n",
      "推論をする\tを て\t推論を 経て\n",
      "研究を始める\tとも とも を を\tマックスプランク研究所とも マックスプランク研究所とも 共同研究を 共同研究を\n",
      "研究を行う\tを\t研究を\n",
      "開発をする\tは は で で を を\t中国では 中国では 官民一体で 官民一体で 研究開発を 研究開発を\n",
      "実験をする\tを\t実験を\n",
      "開発をする\tで を を\t日本で 研究開発を 研究開発を\n",
      "投資をする\tに に に を\t2022年までに 2022年までに 2022年までに 投資を\n",
      "学習をする\tを を\t深層学習を 深層学習を\n",
      "シミュレーションを行う\tを を\t脳シミュレーションを 脳シミュレーションを\n",
      "反乱を起こす\tに対して を\t人間に対して 反乱を\n",
      "弾圧を併せ持つ\tを\t弾圧を\n",
      "監視を行う\tまで まで を に に\t歩行者まで 歩行者まで 監視を 人工知能に 人工知能に\n",
      "手続きを経る\tを を を を\tウイグル族を ウイグル族を 法的手続きを 法的手続きを\n",
      "差別を認める\tを\t差別を\n",
      "研究をする\tを\t研究を\n",
      "展開を変える\tを\t展開を\n",
      "戦争をする\tを\t戦争を\n",
      "制御をする\tは は を を\tAIプログラムは AIプログラムは ファジィ制御を ファジィ制御を\n",
      "判断を介す\tから を\t観点から 判断を\n",
      "禁止を求める\tは は は を\t4月には 4月には 4月には 禁止を\n",
      "運用をめぐる\tを\t運用を\n",
      "競争を行う\tは は は は は を を をめぐって をめぐって\t米国中国ロシアは 米国中国ロシアは 米国中国ロシアは 米国中国ロシアは 米国中国ロシアは 開発競争を 開発競争を 軍事利用をめぐって 軍事利用をめぐって\n",
      "記録をする\tを\t記録を\n",
      "試験を行う\tを\t試験を\n",
      "追及を受ける\tと と で で を\t拒否すると 拒否すると 整合性で 整合性で 追及を\n",
      "研究をする\tが を を\tMicrosoftが 共同研究を 共同研究を\n",
      "解任をする\tは は は を\tGoogle社員らは Google社員らは Google社員らは 解任を\n",
      "解散をする\tは が が が を\tGoogleは 倫理委員会が 倫理委員会が 倫理委員会が 解散を\n",
      "存在を見いだす\tに を を\tものに 霊的存在を 霊的存在を\n",
      "*をする\tを を\t道)」を 道)」を\n",
      "実現をする\tを\t実現を\n",
      "話をする\tは は を\t哲学者は 哲学者は 話を\n",
      "疎通を行う\tを を\t意思疎通を 意思疎通を\n",
      "勘違いをする\tを\t勘違いを\n",
      "議論を行う\tまで まで を\tこれまで これまで 議論を\n"
     ]
    }
   ],
   "source": [
    "for paragraph in paragraphs:\n",
    "    for chunk in paragraph:\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos=='動詞':\n",
    "                verb  = morph.base\n",
    "                lst = []\n",
    "                particles = []\n",
    "                phrase = ''\n",
    "                \n",
    "                for src in chunk.srcs:\n",
    "                    x = paragraph[src].morphs\n",
    "                    for i in range(len(x) - 1):\n",
    "                        if x[i].pos == '名詞' and x[i].pos1 == 'サ変接続': \n",
    "                            if x[i+1].pos == '助詞' and x[i+1].base == 'を': \n",
    "                                phrase  =x[i].base+'を'+verb\n",
    "                        if x[-1].pos == '助詞':\n",
    "                            particles.append(x[-1].base)\n",
    "                            lst.append( ''.join([morph.surface if morph.pos != '記号' else '' for morph in x]))\n",
    "                if phrase != '':\n",
    "                    print(phrase, end='\\t')\n",
    "                    print(*particles, end='\\t')\n",
    "                    print(*lst)\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
