{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "\n",
    "\n",
    "fname_parsed = 'ai.ja/ai.ja.txt.parsed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, dct):\n",
    "        self.surface = dct['surface']\n",
    "        self.base = dct['base']\n",
    "        self.pos = dct['pos']\n",
    "        self.pos1 = dct['pos1']\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"surface:'{}',  base: '{}', pos: '{}',  pos1: '{}'\"\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs     # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst                   # 係り先文節インデックス番号\n",
    "        self.srcs = []                    # 係り元文節インデックス番号のリスト\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"morphs:'{}',  dst: '{}', srcs: '{}'\"\\\n",
    "            .format(self.morphs, self.dst, self.srcs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_analysis(paragraph) -> list:\n",
    "    '''ependency_analysis(係り受け解析)の説明\n",
    "    paragraphはcabocha(CaboCha.FORMAT_LATTICE)で解析したあとのパラグラフとする\n",
    "    例 出力例(道具を用いて『知能』を研究する)\n",
    "    形態素のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）\n",
    "    9.　　 ['道具', 'を'] 10 [5, 6, 8]\n",
    "    10. ['用い', 'て'] 12 [9]\n",
    "    11. ['『', '知能', '』', 'を'] 12 []\n",
    "    12. ['研究', 'する'] 13 [10, 11]\n",
    "    '''\n",
    "    morphs =[]\n",
    "    chunks = []\n",
    "    lines = paragraph.split('\\n')\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "             chunks.append(Chunk(morphs, dst))\n",
    "        elif line[0]=='*':\n",
    "            if len(morphs) > 0:\n",
    "                chunks.append(Chunk(morphs, dst))\n",
    "                morphs =[]\n",
    "            dst = int(line.split(' ')[2].rstrip('D'))\n",
    "        else:\n",
    "            cols = line.split('\\t')\n",
    "            cols = [cols[0]] + cols[1].split(',')\n",
    "            dct = {\n",
    "                'surface': cols[0],\n",
    "                'base': cols[7],\n",
    "                'pos': cols[1],\n",
    "                'pos1': cols[2]\n",
    "            }\n",
    "            morphs.append(Morph(dct))\n",
    "            \n",
    "    for  i, chunk in enumerate(chunks):\n",
    "        if chunk.dst != -1:\n",
    "            chunks[chunk.dst].srcs.append(i)\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# in_fileはテキストファイルをcabocha(CaboCha.FORMAT_LATTICE)で解析した後のテキストファイル\n",
    "with open(fname_parsed) as in_file:\n",
    "    paragraphs = in_file.read().split('EOS\\n')\n",
    "    \n",
    "paragraphs = list(filter(lambda x: x!='', paragraphs))\n",
    "\n",
    "# パラグラフごとに係り受け解析をし, リストに格納\n",
    "paragraphs = [dependency_analysis(paragraph) for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行う \t を に は が で まで て から をめぐって\n",
      "なる \t は に と が で も から て として にとって\n",
      "与える \t に が は を\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for paragraph in paragraphs:\n",
    "    for chunk in paragraph:\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos=='動詞':\n",
    "                verb  = morph.base\n",
    "                # print(verb, end='\\t')\n",
    "                for src in chunk.srcs:\n",
    "                    if paragraph[src].morphs[-1].pos == '助詞':\n",
    "                        particle = paragraph[src].morphs[-1].base\n",
    "                        # print(particle, end=' ')\n",
    "                        if verb not in dic:\n",
    "                            dic[verb] = {particle: 1}\n",
    "                        else: \n",
    "                            dic[verb][particle] = dic[verb].get(particle, 0) + 1\n",
    "                # print()\n",
    "                break\n",
    "                \n",
    "verbs = ['行う', 'なる', '与える']\n",
    "for verb in verbs:\n",
    "    sorted_dic = dict(sorted(dic[verb].items(), key=lambda x:x[1], reverse=True))\n",
    "    print(verb, '\\t', *sorted_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
